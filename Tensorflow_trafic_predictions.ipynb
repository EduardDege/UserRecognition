{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":21,"outputs":[{"output_type":"stream","text":"/kaggle/input/train.csv\n/kaggle/input/test.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in\n\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# Any results you write to the current directory are saved as output.\n# tensorflow hub\nimport tensorflow_hub as hub\n# tensor flow module\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n# matplotlib\nfrom matplotlib import colors\nfrom matplotlib import pyplot as plt\n\n# word vectorizor\n# first converts the text into a matrix of word counts\n# then transforms these counts by normalizing them based on the term frequency\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# used to create word encoders\nfrom sklearn import preprocessing\n\n\n\nclass TFNaiveBayesClassifier:\n    dist = None\n\n    # X is the matrix containing the vectors for each sentence\n    # y is the list target values in the same order as the X matrix\n\n    def fit(self, X, y):\n        unique_y = np.unique(y)  # unique target values: 0,1\n        print(unique_y)\n        # `points_by_class` is a numpy array the size of\n        # the number of unique targets.\n        # in each item of the list is another list that contains the vector\n        # of each sentence from the same target value\n        points_by_class = np.asarray([np.asarray(\n            [np.asarray(\n                X.iloc[x, :]) for x in range(0, len(y)) if y[x] == c]) for c in unique_y])\n        mean_list = []\n        var_list = []\n        for i in range(0, len(points_by_class)):\n            mean_var, var_var = tf.nn.moments(tf.constant(points_by_class[i]), axes=[0])\n            mean_list.append(mean_var)\n            var_list.append(var_var)\n        mean = tf.stack(mean_list, 0)\n        var = tf.stack(var_list, 0)\n        # Create a 3x2 univariate normal distribution with the\n        # known mean and variance\n        self.dist = tfp.distributions.Normal(loc=mean, scale=tf.sqrt(var))\n\n    def predict(self, X):\n        assert self.dist is not None\n        nb_classes, nb_features = map(int, self.dist.scale.shape)\n\n        # uniform priors\n        priors = np.log(np.array([1. / nb_classes] * nb_classes))\n\n        # Conditional probabilities log P(x|c)\n        # (nb_samples, nb_classes, nb_features)\n        all_log_probs = self.dist.log_prob(\n            tf.reshape(\n                tf.tile(X, [1, nb_classes]), [-1, nb_classes, nb_features]))\n        # (nb_samples, nb_classes)\n        cond_probs = tf.reduce_sum(all_log_probs, axis=2)\n\n        # posterior log probability, log P(c) + log P(x|c)\n        joint_likelihood = tf.add(priors, cond_probs)\n\n        # normalize to get (log)-probabilities\n        norm_factor = tf.reduce_logsumexp(\n            joint_likelihood, axis=1, keepdims=True)\n        log_prob = joint_likelihood - norm_factor\n        # exp to get the actual probabilities\n        return tf.exp(log_prob)\n    \ndef initModel(X_train_matrix,X_test_matrix,y_train,train_df):\n    # Here we initialize our naive bayes model and fit it using the training data\n    tf_nb = TFNaiveBayesClassifier()\n    tf_nb.fit(pd.DataFrame(X_train_matrix), y_train)\n    # predict probability of each target values in the test set\n    y_pred = tf_nb.predict(X_test_matrix)\n    # Create a dataframe containing the probability of each target given the text in each tweet.\n    predProbGivenText_df = pd.DataFrame(y_pred.numpy())\n    predProbGivenText_df.head()\n    uniq_keywords = train_df[\"keyword\"].unique()[1:]\n    print(len(uniq_keywords))\n    print(uniq_keywords)\n\ndef getProbality(train_df):\n    uniq_keywords = train_df[\"keyword\"].unique()[1:]\n    kword_resArr = []\n    print(len(uniq_keywords))\n    for kword in uniq_keywords:\n        kword_df = train_df.loc[train_df[\"keyword\"] == kword,: ]\n        total_kword = float(len(kword_df))\n        target0_n = float(len(kword_df.loc[kword_df[\"target\"]==0,:]))\n        target1_n = float(len(kword_df.loc[kword_df[\"target\"]==1,:]))\n        kword_prob_df = pd.DataFrame({'keyword':[kword],\n                                 \"keywordPred0\": [target0_n/total_kword],\n                                 \"keywordPred1\": [target1_n/total_kword]})\n        kword_resArr.append(kword_prob_df)\n    predProbGivenKeyWord_df = pd.concat(kword_resArr)\n    predProbGivenKeyWord_df.head()\n    print(predProbGivenKeyWord_df.head())\n    \n\ndef main():\n    train_df = pd.read_csv(\"/kaggle/input/train.csv\")\n    test_df = pd.read_csv(\"/kaggle/input/train.csv\")\n    train_df.groupby(\"target\")[\"id\"].nunique()\n    # print(len(test_df.loc[test_df[\"location\"].notnull(), \"location\"]))\n    # print(len(test_df.loc[test_df[\"location\"].notnull(), \"location\"].unique()))\n    # print(len(test_df.loc[test_df[\"keyword\"].notnull(), \"keyword\"]))\n    # print(len(test_df.loc[test_df[\"keyword\"].notnull(), \"keyword\"].unique()))\n    # Tweet Text: Sentence Embedding\n    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/3\")\n    # print(embed)\n    X_train_embeddings = embed(train_df[\"text\"].values)\n    X_test_embeddings = embed(test_df[\"text\"].values)\n    # Tensor Flow Naiver Bayes\n    X_train_matrix = X_train_embeddings['outputs'].numpy()\n    X_test_matrix = X_test_embeddings['outputs'].numpy()\n    y_train = tf.constant(train_df[\"target\"])\n    initModel(X_train_matrix,X_test_matrix,y_train,train_df)\n    getProbality(train_df)\n    \n\n\n\n\nif __name__ == '__main__':\n    main()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}